
## ðŸ”¹ Step 1: Define a Schema

```python
schema = [
    ResponseSchema(name='fact_1', description='Fact 1 about the topic'),
    ResponseSchema(name='fact_2', description='Fact 2 about the topic'),
    ResponseSchema(name='fact_3', description='Fact 3 about the topic'),
]
```

* A **`ResponseSchema`** tells the model how we want the output to be structured.
* Here, we want **3 facts** about a topic, so we define:

  * `fact_1` â†’ First fact
  * `fact_2` â†’ Second fact
  * `fact_3` â†’ Third fact

---

## ðŸ”¹ Step 2: Create a Parser

```python
parser = StructuredOutputParser.from_response_schemas(schema)
```

* This parser will enforce that the modelâ€™s output **follows the schema**.
* For example, it ensures the output looks like JSON with keys `"fact_1"`, `"fact_2"`, `"fact_3"`.

---

## ðŸ”¹ Step 3: Define the Prompt

```python
template = PromptTemplate(
    template='Give 3 fact about {topic} \n {format_instructions}',
    input_variables=['topic'],
    partial_variables={'format_instructions':parser.get_format_instructions()}
)
```

* `{topic}` â†’ placeholder for what subject we want facts about.
* `{format_instructions}` â†’ automatically inserted instructions from the parser, telling the LLM **how to format the output** (usually JSON).

ðŸ‘‰ Example final prompt when `topic="black hole"`:

```
Give 3 fact about black hole
Format your response as a JSON object with keys:
- fact_1: Fact 1 about the topic
- fact_2: Fact 2 about the topic
- fact_3: Fact 3 about the topic
```

---

## ðŸ”¹ Step 4: Create the Chain

```python
chain = template | model | parser
```

* `template` â†’ fills in the prompt
* `model` â†’ runs the LLM
* `parser` â†’ parses the output into a **Python dictionary**

---

## ðŸ”¹ Step 5: Run the Chain

```python
result = chain.invoke({'topic':'black hole'})
print(result)
```

* Input: `{"topic": "black hole"}`
* Model generates structured JSON (facts).
* Parser converts it into a Python dict.

âœ… Example Output:

```python
{
  'fact_1': 'A black hole has such strong gravity that not even light can escape.',
  'fact_2': 'They are formed from the collapse of massive stars.',
  'fact_3': 'Black holes can grow by absorbing nearby matter and merging with other black holes.'
}
```

---

### ðŸ”‘ In Short:

This code ensures that instead of random text, the LLM always gives **well-structured output** (facts inside a dict) thatâ€™s easy to use in your program. ðŸš€

---



<img width="1190" height="152" alt="image" src="https://github.com/user-attachments/assets/d04f951a-80f7-419e-bb1b-aabe1972edec" />
